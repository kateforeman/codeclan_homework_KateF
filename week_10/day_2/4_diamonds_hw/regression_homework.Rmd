---
title: "R Notebook"
output: html_notebook
---

1. Read in the data. 

```{r}
library(tidyverse) 

diamonds <- read_csv("diamonds.csv") 

head(diamonds) 
```

2. We expect the carat of the diamonds to be strong correlated with the physical dimensions x, y and z. Use ggpairs() to investigate correlations between these 
four variables. 

It seems the individual features each have strong correlations with the carat. 

```{r}
library(GGally) 

ggpairs(diamonds) 
```

3. So, we do find significant correlations. Let’s drop columns x, y and z from the dataset, in preparation to use only carat going forward. 

```{r}
diamonds <- diamonds %>% 
  select(-c(x, y, z)) 
```

4. We are interested in developing a regression model for the price of a 
diamond in terms of the possible predictor variables in the dataset.

i) Use ggpairs() to investigate correlations between price and the predictors 
(this may take a while to run, don’t worry, make coffee or something). 

There is a strong correlation of 0.922 between the price and carat. 

```{r}
ggpairs(diamonds) 
```

ii) Perform further ggplot visualisations of any significant correlations you 
find. 

```{r}
diamonds %>% 
  ggplot(aes(price, carat)) + 
  geom_point() 
```

5. Shortly we may try a regression fit using one or more of the categorical 
predictors cut, clarity and color, so let’s investigate these predictors: 

i) Investigate the factor levels of these predictors. How many dummy variables 
do you expect for each of them? 

Cut = 4 
Clarity = 7 
Color = 6 


```{r}
unique(diamonds$clarity) 
```

ii) Use the dummy_cols() function in the fastDummies package to generate dummies for these predictors and check the number of dummies in each case. 

```{r}
library(fastDummies)

diamonds %>% 
  dummy_cols(select_columns = c("cut", "clarity", "color"), remove_first_dummy = 
              TRUE, remove_selected_columns = TRUE) 
```

6. Going forward we’ll let R handle dummy variable creation for categorical 
predictors in regression fitting (remember lm() will generate the correct
numbers of dummy levels automatically, absorbing one of the levels into the
intercept as a reference level). 

i) First, we’ll start with simple linear regression. Regress price on carat 
and check the regression diagnostics. 

```{r}
library(ggfortify) 

model <- lm(price ~ carat, data = diamonds) 

autoplot(model)
```

ii) Run a regression with one or both of the predictor and response variables log() transformed and recheck the diagnostics. Do you see any improvement? 

```{r}
diamonds %>% 
  log(carat) 
```


```{r}

```

ii) Let’s use log() transformations of both predictor and response. Next, 
experiment with adding a single categorical predictor into the model. Which 
categorical predictor is best? [Hint - investigate r2 values] 

```{r}
diamonds_logs <- log(diamonds_log$price) 
```

